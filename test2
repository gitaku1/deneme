from dataiku.core.model_provider import get_model_from_cache
from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering

# Model ismini kullan – Dataiku Cache içinde bu modelin import edilmiş olması gerekiyor
model_name = "deepset/bert-base-cased-squad2"

# Cache’den modelin path’ini al
model_path = get_model_from_cache(model_name)

# Cache’den model ve tokenizer'ı yükle
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
model = AutoModelForQuestionAnswering.from_pretrained(model_path, local_files_only=True)

# Pipeline oluştur
qa = pipeline("question-answering", model=model, tokenizer=tokenizer)

# Prompt denemesi
context = "Dataiku DSS is a collaborative data science platform."
question = "What kind of platform is Dataiku DSS?"

result = qa({
    "context": context,
    "question": question
})

print("Answer:", result.get("answer"))
print("Score:", result.get("score"))
print("Start:", result.get("start"))
print("End:", result.get("end"))







from dataiku.core.model_provider import get_model_from_cache
from transformers import MarianMTModel, MarianTokenizer, pipeline

# Model adı
model_name = "Helsinki-NLP/opus-mt-tc-big-en-tr"

# Dataiku cache'den model yolu al
model_path = get_model_from_cache(model_name)

# Tokenizer ve modeli yükle (yalnızca yerel dosyalar)
tokenizer = MarianTokenizer.from_pretrained(model_path, local_files_only=True)
model = MarianMTModel.from_pretrained(model_path, local_files_only=True)

# Pipeline oluştur
translator = pipeline("translation", model=model, tokenizer=tokenizer)

# Örnek metin çevirisi
input_text = "I know Tom didn't want to eat that."

translated = translator(input_text)

print("Input:", input_text)
print("Translation:", translated[0]['translation_text'])











# 6. Sonucu yazdır
print(outputs[0]["generated_text"])
