import pyarrow.parquet as pq

try:
    # Tüm row group'ları teker teker okuyarak kurtarma
    pf = pq.ParquetFile("bozuk.parq")
    for i in range(pf.num_row_groups):
        try:
            batch = pf.read_row_group(i)
            pq.write_table(batch, f"repaired_part_{i}.parq")
            print(f"✅ RowGroup {i} kurtarıldı ({batch.num_rows} satır)")
        except Exception as e:
            print(f"❌ RowGroup {i} atlandı: {str(e)}")
except Exception as e:
    print(f"🚨 Dosya açılamadı: {str(e)}")



from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("RepairParquet").getOrCreate()

try:
    df = spark.read.parquet("hdfs:///hdfs/path/to/broken.parq")
    df.limit(100).show()  # varsa veri döner
    df.write.mode("overwrite").parquet("hdfs:///repaired/path/broken_fixed.parq")
    print("✅ Parquet dosyası yeniden yazıldı")
except Exception as e:
    print("❌ Okunamıyor:", str(e))
