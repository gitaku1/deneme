import pyarrow.parquet as pq

try:
    # TÃ¼m row group'larÄ± teker teker okuyarak kurtarma
    pf = pq.ParquetFile("bozuk.parq")
    for i in range(pf.num_row_groups):
        try:
            batch = pf.read_row_group(i)
            pq.write_table(batch, f"repaired_part_{i}.parq")
            print(f"âœ… RowGroup {i} kurtarÄ±ldÄ± ({batch.num_rows} satÄ±r)")
        except Exception as e:
            print(f"âŒ RowGroup {i} atlandÄ±: {str(e)}")
except Exception as e:
    print(f"ğŸš¨ Dosya aÃ§Ä±lamadÄ±: {str(e)}")



from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("RepairParquet").getOrCreate()

try:
    df = spark.read.parquet("hdfs:///hdfs/path/to/broken.parq")
    df.limit(100).show()  # varsa veri dÃ¶ner
    df.write.mode("overwrite").parquet("hdfs:///repaired/path/broken_fixed.parq")
    print("âœ… Parquet dosyasÄ± yeniden yazÄ±ldÄ±")
except Exception as e:
    print("âŒ OkunamÄ±yor:", str(e))
