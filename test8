import pyarrow.fs as fs
from concurrent.futures import ThreadPoolExecutor, as_completed

# HDFS connection (adjust namenode host:port)
hdfs, _ = fs.HadoopFileSystem.from_uri("hdfs://namenode:8020")

# HDFS directory to scan
HDFS_DIR = "/user/ali"

# Number of parallel threads
MAX_WORKERS = 10

def is_corrupt(path):
    """Check if a file is corrupt (Parquet/rowfile check via first/last 4 bytes)"""
    try:
        with hdfs.open_input_file(path) as f:
            size = f.size
            if size < 8:
                return path  # too small, consider corrupt

            # Read first 4 bytes
            head = f.read(4)
            # Read last 4 bytes
            f.seek(size - 4)
            tail = f.read(4)

            if head != b'PAR1' or tail != b'PAR1':
                return path  # corrupt
    except Exception as e:
        return path  # if any error occurs, mark as corrupt
    return None  # file is OK

def list_files_recursively(hdfs_dir):
    """Recursively list all files under HDFS_DIR"""
    files = []
    for info in hdfs.get_file_info(fs.FileSelector(hdfs_dir, recursive=True)):
        if info.is_file:
            files.append(info.path)
    return files

def main():
    files = list_files_recursively(HDFS_DIR)
    corrupt_files = []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_file = {executor.submit(is_corrupt, f): f for f in files}
        for future in as_completed(future_to_file):
            result = future.result()
            if result:
                corrupt_files.append(result)

    print("CORRUPT files:")
    for f in corrupt_files:
        print(f)

if __name__ == "__main__":
    main()
